# Learning-Fast-Surrogates-for-2-D-Airfoil-Aerodynamics-with-FNO,-UNet,-POD and Physics-Aware Fine-Tuning
Fast, reproducible surrogates for 2-D airfoil aerodynamics (FNO/UNet/POD) with physics-aware fine-tuning, dataset-level benchmarks, and paper-ready plots.

# Abstract
High-fidelity CFD delivers accurate aerodynamic predictions but remains too slow for iterative design or real-time deployment. We develop and compare three surrogates for steady 2-D external aerodynamics—Fourier Neural Operators (FNO), convolutional UNets, and a Proper Orthogonal Decomposition (POD) regressor—trained on an AirFrans-style dataset. Inputs are a signed-distance field, a binary airfoil mask, angle of attack, and freestream speed; outputs are velocity components u,v and static pressure p on a 128×256 Cartesian grid. We introduce a physics-aware fine-tuning recipe that augments the data loss with gradient, vorticity, and divergence penalties, and applies edge-focused weighting near the airfoil surface. After fine-tuning, dataset-level normalized RMSE typically stabilizes around u≈0.19, v≈0.03, p≈0.027, with case-wise consistency across models. UNet and FNO capture velocity structures most faithfully, while POD achieves the lowest pressure error, albeit with slight wake smoothing. The result is a set of fast, reproducible surrogates with materially improved near-surface and wake fidelity suitable for downstream evaluation and benchmarking.

# 1. Introduction
Aerodynamic design sits at the intersection of accuracy and speed. Traditional CFD provides gold-standard predictions, but cost grows prohibitive when exploring large design spaces or delivering real-time feedback in digital twins and flight simulators. Data-driven surrogates promise near-instant evaluation by learning the input–output map from curated simulations. Among the many choices, three classes offer a complementary mix of inductive biases and practicality: neural operators (FNO) for long-range coupling, convolutional encoder–decoders (UNets) for sharp local structures, and reduced-order models (POD) for stability and interpretability. Our goal is pragmatic: produce full-field surrogates for (u,v,p) that evaluate in milliseconds, report dataset-level metrics honestly, and visualize where models succeed or fail. To close the gap between data fit and physics fidelity, we add a simple but effective physics-aware fine-tuning stage that measurably sharpens gradients and reduces wake/near-wall errors.

# 2. Governing fields, data, and normalization
The targets are steady velocity components u(x,y),v(x,y) in m/s and static pressure p(x,y) in Pa. Inputs include the signed-distance field ϕ(x,y), a binary airfoil mask M(x,y), the angle of attack α (encoded as a constant plane), and freestream speed U∞ (also as a constant plane). All arrays are defined on a uniform H×W=128×256 lattice in the body-fixed frame. Each training run persists per-channel means and standard deviations in out_norm.npz; evaluation inverse-normalizes predictions using the emitting run’s statistics to keep outputs in physical units. Ground truth is not re-scaled at evaluation time, avoiding double normalization.

# 3. Models in depth
UNet. The UNet is a fully convolutional encoder–decoder with skip connections. Downsampling stacks compress spatial context while upsampling decoders fuse coarse features with high-resolution skip activations. For aerodynamics this behaves like a hierarchy of learned finite-difference stencils: near-wall shear, separation bubbles, and wake rolls emerge naturally. Its locality bias yields strong near-field velocity reconstructions.
FNO. The Fourier Neural Operator transforms features to Fourier space, linearly mixes a band of low/mid spectral modes with learned complex weights, and returns to the spatial domain. This enforces a bias toward nonlocal coupling: perturbations at the leading edge propagate downstream through global spectral modes. In incompressible regimes, where pressure communicates globally, FNO is particularly effective at recovering pressure gradients and coherent wakes.
POD regressor. Proper Orthogonal Decomposition factorizes snapshot matrices into orthogonal spatial modes. Truncating to the leading r modes yields a low-rank prior: reconstructions are smooth and denoised. We learn a regressor from inputs to modal coefficients and reconstruct fields as a linear combination of precomputed modes. POD offers stability and interpretability; its limitation is under-representing sharp gradients that sit outside the truncated subspace.

# 4. Physics-aware fine-tuning
To improve fidelity without destabilizing training, we fine-tune pretrained surrogates with additional physics-motivated terms. The data term is a per-channel L1 loss on (u,v,p). A gradient loss aligns ∇u,∇v,∇p via finite-difference operators, sharpening suction peaks and wakes. A vorticity loss aligns ω=∂v/∂x−∂u/∂y, suppressing spurious rotational artefacts. A light divergence loss encourages approximate incompressibility. Finally, we apply edge weighting using a decaying function of distance to the airfoil surface so pixels controlling forces receive higher weight. In practice, this recipe converges in tens of epochs, reduces over-smoothness in UNet/POD, and damps ringing in FNO near steep gradients.

# 5. Why physics-aware fine-tuning matters (implementation details)
Fine-tuning is applied uniformly to FNO, UNet, and POD with the same minibatch pipeline as base training. Spatial derivatives are computed with centered differences on the uniform grid. The edge weight is constructed from the signed-distance field using a shallow exponential window so the emphasis is highest inside a few pixels of the surface and decays into the far field. Hyperparameters are limited to a handful of scalars weighting the gradient, vorticity, and divergence terms, plus two numbers shaping the edge window; cosine learning-rate decay with a small floor avoids over-stepping late in training. We also support a simple affine calibration pass (optional) that regresses a per-channel linear correction on a held-out split; this often trims residual bias in p with negligible cost. The key properties of this approach are simplicity (no new networks), locality (losses reflect physics operators), and robustness (the same settings work across architectures).

# 6. Results and qualitative analysis
After fine-tuning, dataset-level normalized RMSE typically stabilizes around u≈0.19, v≈0.03, and p≈0.027. Qualitatively, error maps concentrate at stagnation points, on suction peaks, and along the wake shear layer—precisely where gradients are steep. UNet and FNO best preserve near-wall and wake structures; POD achieves competitive pressure accuracy but smooths small vortices and shear layers, reflecting the low-rank truncation. Cropped-window metrics (e.g., removing an 8-pixel boundary) further reduce pressure error, indicating that most remaining pressure bias sits near the outer domain where boundary handling is hardest.

# 7. Why u is hardest to predict
The x-velocity u exhibits the largest normalized error across models. Unlike v, which is small outside boundary layers, u spans stagnation to freestream speeds and transitions through acceleration and deceleration zones. This broad dynamic range amplifies small systematic errors. Momentum coupling to pressure further magnifies mistakes in ∇p, and finite spectral/low-rank truncation under-resolves steep u gradients at the suction peak and in separated wakes. Physics-aware fine-tuning mitigates these effects by explicitly supervising derivatives and vorticity near the airfoil, but some residual u error is intrinsic to the setting without multi-resolution supervision.

# 8. What is new in this work
Two elements distinguish this project. First, a unified, reproducible evaluation harness that inverse-normalizes with run-specific statistics, reports dataset-level metrics (MAE, RMSE, NRMSE, correlation), and emits standardized montages of fields and errors. Second, a minimal physics-aware fine-tuning recipe that measurably sharpens near-surface features and wakes across architectures without architectural surgery. Together they move the surrogates from “good pictures” to physically more faithful predictions suitable for benchmarking and downstream integration.

# 9. Limitations and future directions
Despite improvements, u remains the most challenging variable due to range and gradient sensitivity. POD’s performance depends strongly on the chosen basis and mode count; mismatched bases induce smoothing. Our current evaluation emphasizes field metrics; for design decisions, integral coefficients CL and CD, pressure coefficient curves, and speedups versus CFD should accompany field errors. Future work will add multi-resolution supervision, boundary-aware operators, and explicit coefficient losses. We also plan to systematize affine calibration and extend the pipeline to transient cases and 3-D wing sections.

# 10. Conclusion
We presented FNO, UNet, and POD surrogates for steady 2-D airfoil flows and showed that a simple physics-aware fine-tuning stage consistently improves gradient-sensitive regions while retaining stability. The models evaluate orders of magnitude faster than high-fidelity solvers and produce fields in physical units with a reproducible, honest evaluation protocol. These properties make them practical building blocks for design loops, data augmentation, and reduced-order digital twins.

# 11. Reproducibility notes
Each run writes out_norm.npz with per-channel mean and standard deviation; evaluation always inverse-normalizes predictions with this file. Ground-truth targets from the dataset remain in physical units. All reported metrics are dataset-level unless otherwise stated; single-case montages are for qualitative inspection only. Fine-tuning uses the same normalization as the base run and retains the original checkpoint format so existing plotting scripts work unchanged.

# Repository pointers.
Training and fine-tuning scripts prepare inputs as [ϕ,M,α,U∞] and predict [u,v,p]. Plotting utilities generate comparable GT vs. prediction montages with shared color scales and optional error panels. Benchmark scripts traverse the test split, accumulate per-channel metrics, and save CSV tables plus summary plots. All scripts default to the run-local normalizer to preserve units and avoid cross-run scaling drift.


